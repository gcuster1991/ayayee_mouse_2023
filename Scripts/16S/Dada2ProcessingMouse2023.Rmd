---
title: "Ayayee Mouse 2023 data processing"
output: html_notebook
---

```{r}
library(rlang)
library(dada2) 
library(ShortRead)
```

set pathway to demultiplexed data.
```{r}
path_dm <- ("/Users/gordoncuster/Desktop/paul/")
```

get forward and reverse file names
```{r}
# Get full paths for all files and save them for downstream analyses
# Forward and reverse fastq filenames have format: 
fnFs <- sort(list.files(path_dm, pattern="R1_", full.names = TRUE))
fnRs <- sort(list.files(path_dm, pattern="R2_", full.names = TRUE))

sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

set pathway to cutadapt and identify primer sequences
```{r}
# Set up pathway to cutadapt (primer trimming tool) and test
cutadapt <- "/Users/gordoncuster/.local/bin/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version") # Run shell commands from R

# Set up the primer sequences to pass along to cutadapt
FWD <- "GTGYCAGCMGCCGCGGTAA"  ## this is 515f
#TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG-GTGCCAGCMGCCGCGG TAA
REV <- "CCGTCAATTCMTTTRAGTTT"  ## this is 907r

  #GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG-CCGTCAATTCMTTTRAGTTT
```

function for identifying primers in sequence reads
```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
# Write a function that counts how many time primers appear in a sequence
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
```

create subdirectory for filtered reads. Here, we remove any reads with N basepairs. 
```{r}
# Name the N-filtered files to put them in filtN/ subdirectory
fnFs.filtN <- file.path(path_dm, "filtN", basename(fnFs))
fnRs.filtN <- file.path(path_dm, "filtN", basename(fnRs))

# Filter Ns from reads and put them into the filtN directory
out_pre<-filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, trimLeft = 20, trimRight = 20, multithread = TRUE) 
```

Count primers in "N-filtered" reads
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[2]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[2]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[2]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[2]]))

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[7]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[7]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[7]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[7]]))

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[8]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[8]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[8]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[8]]))
```

Run cutadapt to remove primers 
```{r}
path.cut <- file.path(path_dm, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input file
}
```

Check for primer removal. 
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[2]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[2]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[2]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[2]]))

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[7]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[7]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[7]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[7]]))

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[8]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[8]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[8]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[8]]))


```
get names and pathways of cutadapt trimmmed reads. 
```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "R1_", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R2_", full.names = TRUE))

# Extract sample names, assuming filenames have format:
#get.sample.name <- function(fname) strsplit(basename(fname), "R1_")[[1]][2]
#sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```

Plot quality profiles to identify filtering cutoffs. 
```{r}
plotQualityProfile(cutFs[1])
plotQualityProfile(cutRs[1])

plotQualityProfile(cutFs[8])
plotQualityProfile(cutRs[8])

plotQualityProfile(cutFs[7])
plotQualityProfile(cutRs[7])

```

new folder for filtered reads. 
```{r}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
```

Quality filter and plot filtered read quality profiles.
```{r}
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, truncQ = 2, trimRight =  c(10,30),
              maxN=0, maxEE=c(2,2), rm.phix=TRUE,
              compress=TRUE, multithread=TRUE)

# On Windows set multithread=FALSE
head(out)

plotQualityProfile(filtFs[1:4])
plotQualityProfile(filtRs[1:4])
```

Learn errors
```{r}
errF <- learnErrors(filtFs, nbases = 1e9, multithread = TRUE)
errR <- learnErrors(filtRs, nbases = 1e9, multithread = TRUE)
```

Check error plots
```{r}
plotErrors(errF, nominalQ = TRUE)
plotErrors(errR, nominalQ = TRUE)
```

#derereplicate
```{#r}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

correct via dada algo. 
```{r}
dadaFs <- dada(filtFs, err = errF, multithread = TRUE, selfConsist = T)
dadaRs <- dada(filtRs, err = errR, multithread = TRUE, selfConsist = T)
```

merge paired reads with a min overlap of 10. 
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE, returnRejects = T)
```

Make sequence table and remove all reads that are not  bp in length. 
```{r}
seqtab_orig <- makeSequenceTable(mergers)
dim(seqtab_orig)

table(nchar(getSequences(seqtab_orig)))

#seqtab <- seqtab_orig[,nchar(colnames(seqtab_orig)) %in% 370:378]
#dim(seqtab)
```

Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab_orig, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
table(nchar(getSequences(seqtab.nochim)))
#peak around 376
plot(table(nchar(getSequences(seqtab.nochim))))
seqtab <- seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% 367:375]
dim(seqtab)
```

Count reads passing through pipeline
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, 
    getN), rowSums(seqtab_orig),  rowSums(seqtab.nochim), rowSums(seqtab))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "seqtaborig", 
    "nonchim", "seqtab_trimmed")
rownames(track) <- sample.names
head(track)

#34% retained in total
colSums(track)[ncol(track)]/colSums(track)[1]
#45 for the CV
colSums(track[c(1:5),])[ncol(track[c(1:5),])]/colSums(track[c(1:5),])[1]
#21% for the GF
colSums(track[c(6:11),])[ncol(track[c(6:11),])]/colSums(track[c(6:11),])[1]

track
```

Assign taxonomy with the silva species level database. 
```{r}
taxa <- assignTaxonomy(seqtab, "/Volumes/GoogleDrive/My Drive/databases/silva_nr99_v138.1_wSpecies_train_set.fa.gz", multithread=TRUE)
#taxa <- assignTaxonomy(seqtab.nochim, "/Users/gordoncuster/Desktop/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)
#taxa <- addSpecies(taxa, "/Users/gordoncuster/Desktop/silva_species_assignment_v138.1.fa.gz")
```

```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

create phylogenetic tree with phangorn defaults for unifrac distances. 
```{r}
library(phangorn)
library(DECIPHER)

sequences<-getSequences(seqtab)
names(sequences)<-sequences
#Run Sequence Alignment (MSA) using DECIPHER

alignment <- AlignSeqs(DNAStringSet(sequences), anchor=NA)
#Change sequence alignment output into a phyDat structure

phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
#Create distance matrix

dm <- dist.ml(phang.align)
#Perform Neighbor joining

treeNJ <- NJ(dm) # Note, tip order != sequence order
#Internal maximum likelihood

fit = pml(treeNJ, data=phang.align)
#negative edges length changed to 0!

fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))
```

```{r}
md<-read.csv("/Users/gordoncuster/Desktop/Git_Projects/ayayee_mouse_2023/Data/Mice_csia_mapping.csv")
```

